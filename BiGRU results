{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11111498,"sourceType":"datasetVersion","datasetId":6926583},{"sourceId":11120959,"sourceType":"datasetVersion","datasetId":6934896},{"sourceId":11130665,"sourceType":"datasetVersion","datasetId":6941960},{"sourceId":306041,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":261095,"modelId":282249}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mirnasherif/bigru-results?scriptVersionId=230284837\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-28T19:28:31.654371Z","iopub.execute_input":"2025-03-28T19:28:31.65481Z","iopub.status.idle":"2025-03-28T19:28:33.526862Z","shell.execute_reply.started":"2025-03-28T19:28:31.654763Z","shell.execute_reply":"2025-03-28T19:28:33.525596Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Input, Embedding, LSTM, Bidirectional, Dense, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T19:28:33.52832Z","iopub.execute_input":"2025-03-28T19:28:33.528899Z","iopub.status.idle":"2025-03-28T19:28:54.74597Z","shell.execute_reply.started":"2025-03-28T19:28:33.528858Z","shell.execute_reply":"2025-03-28T19:28:54.744859Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load dataset\ndf = pd.read_csv(\"/kaggle/input/jigsaw-competition-merged-and-balanced/cleaned_data.csv\")\nlabels = [\"toxic\", \"severe_toxic\", \"obscene\", \"insult\", \"threat\", \"identity_hate\"]  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T19:28:54.748305Z","iopub.execute_input":"2025-03-28T19:28:54.749012Z","iopub.status.idle":"2025-03-28T19:29:26.694182Z","shell.execute_reply.started":"2025-03-28T19:28:54.748966Z","shell.execute_reply":"2025-03-28T19:29:26.693015Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T19:29:26.696222Z","iopub.execute_input":"2025-03-28T19:29:26.696628Z","iopub.status.idle":"2025-03-28T19:29:26.727678Z","shell.execute_reply.started":"2025-03-28T19:29:26.696596Z","shell.execute_reply":"2025-03-28T19:29:26.726593Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T19:29:26.728704Z","iopub.execute_input":"2025-03-28T19:29:26.729001Z","iopub.status.idle":"2025-03-28T19:29:26.759077Z","shell.execute_reply.started":"2025-03-28T19:29:26.728967Z","shell.execute_reply":"2025-03-28T19:29:26.758019Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df=df.dropna()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T19:29:26.760155Z","iopub.execute_input":"2025-03-28T19:29:26.760484Z","iopub.status.idle":"2025-03-28T19:29:27.582647Z","shell.execute_reply.started":"2025-03-28T19:29:26.76043Z","shell.execute_reply":"2025-03-28T19:29:27.58176Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\n\n# Function to check if a sentence contains special characters\ndef has_special_chars(text):\n    return bool(re.search(r'[^a-zA-Z0-9\\s]', text))  # Returns True if special chars exist\n\n# Filter sentences with special characters\nspecial_char_sentences = df[df['comment_text'].apply(has_special_chars)]\n\n# Display them\nprint(special_char_sentences)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T19:29:27.583719Z","iopub.execute_input":"2025-03-28T19:29:27.584084Z","iopub.status.idle":"2025-03-28T19:29:40.476377Z","shell.execute_reply.started":"2025-03-28T19:29:27.584047Z","shell.execute_reply":"2025-03-28T19:29:40.475319Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\n\n# Function to remove special characters\ndef remove_special_chars(text):\n    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Keeps only letters, numbers, and spaces\n\n# Apply to your dataframe\ndf['comment_text'] = df['comment_text'].astype(str).apply(remove_special_chars)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T19:29:40.477675Z","iopub.execute_input":"2025-03-28T19:29:40.478277Z","iopub.status.idle":"2025-03-28T19:29:55.112855Z","shell.execute_reply.started":"2025-03-28T19:29:40.478235Z","shell.execute_reply":"2025-03-28T19:29:55.111596Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\n\n# Load the tokenizer\nwith open('/kaggle/input/hatedetectiontokenizer/tokenizer.pkl', 'rb') as f:\n    tokenizer = pickle.load(f)\n\n# Confirm tokenizer loaded correctly\nprint(f\"Tokenizer vocabulary size: {len(tokenizer.word_index)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T19:29:55.116677Z","iopub.execute_input":"2025-03-28T19:29:55.116988Z","iopub.status.idle":"2025-03-28T19:29:56.133009Z","shell.execute_reply.started":"2025-03-28T19:29:55.116961Z","shell.execute_reply":"2025-03-28T19:29:56.131614Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n# Load the precomputed embedding matrix\nembedding_matrix = np.load('/kaggle/input/embeddingmatrix/embedding_matrix.npy')\n\n# Check the shape\nprint(f\"Embedding Matrix Shape: {embedding_matrix.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T19:29:56.134705Z","iopub.execute_input":"2025-03-28T19:29:56.134998Z","iopub.status.idle":"2025-03-28T19:30:04.647551Z","shell.execute_reply.started":"2025-03-28T19:29:56.134973Z","shell.execute_reply":"2025-03-28T19:30:04.646369Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T19:30:04.648402Z","iopub.execute_input":"2025-03-28T19:30:04.64879Z","iopub.status.idle":"2025-03-28T19:30:04.653948Z","shell.execute_reply.started":"2025-03-28T19:30:04.64876Z","shell.execute_reply":"2025-03-28T19:30:04.652725Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vocab_size=478019\nprint(f\"Vocabulary size: {vocab_size}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T19:30:04.655183Z","iopub.execute_input":"2025-03-28T19:30:04.655545Z","iopub.status.idle":"2025-03-28T19:30:04.679106Z","shell.execute_reply.started":"2025-03-28T19:30:04.655499Z","shell.execute_reply":"2025-03-28T19:30:04.677661Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"fit_on_texts(texts): Learns word mappings and counts vocabulary size.\ntexts_to_sequences(texts): Converts each text into a sequence of numbers.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nMAX_WORDS = vocab_size\nMAX_SEQUENCE_LENGTH = 100  # Use 95th percentile length :86\nSTRIDE =50   # Fix stride calculation\nEMBEDDING_DIM = 300  # Keep the same unless changing embeddings\n\n\ntexts = df[\"comment_text\"].astype(str).values  # Convert comments to string\nsequences = tokenizer.texts_to_sequences(texts)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T19:30:04.680428Z","iopub.execute_input":"2025-03-28T19:30:04.68088Z","iopub.status.idle":"2025-03-28T19:32:08.710722Z","shell.execute_reply.started":"2025-03-28T19:30:04.680812Z","shell.execute_reply":"2025-03-28T19:32:08.709772Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sliding Window Function\ndef create_sliding_window_sequences(sequences, max_len=MAX_SEQUENCE_LENGTH, stride=50):\n    all_chunks = []\n    for seq in sequences:\n        chunks = []\n        if len(seq) <= max_len:\n            seq += [0] * (max_len - len(seq))\n            chunks.append(seq)\n        else:\n            for i in range(0, len(seq) - max_len + 1, stride):\n                chunks.append(seq[i : i + max_len])\n            if (len(seq) - max_len) % stride != 0:\n                chunks.append(seq[-max_len:])\n        all_chunks.extend(chunks)\n    return np.array(all_chunks)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T19:32:08.711745Z","iopub.execute_input":"2025-03-28T19:32:08.712181Z","iopub.status.idle":"2025-03-28T19:32:08.719136Z","shell.execute_reply.started":"2025-03-28T19:32:08.712141Z","shell.execute_reply":"2025-03-28T19:32:08.717681Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Apply sliding window\nX_sliding = create_sliding_window_sequences(sequences)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T19:32:08.720063Z","iopub.execute_input":"2025-03-28T19:32:08.720416Z","iopub.status.idle":"2025-03-28T19:33:03.072466Z","shell.execute_reply.started":"2025-03-28T19:32:08.720373Z","shell.execute_reply":"2025-03-28T19:33:03.071253Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Ensure Y is a 1D array\nY = df[labels].values  # Shape: (4703664, 6)\n\n# Count how many chunks per original sequence\nnum_chunks_per_sequence = [len(create_sliding_window_sequences([seq])) for seq in sequences]\n\n# Repeat labels correctly\nY_expanded = np.repeat(Y, num_chunks_per_sequence, axis=0)\n\n# Verify shapes again\nprint(f\"Fixed Shapes - X_sliding: {X_sliding.shape}, Y_expanded: {Y_expanded.shape}\")\nassert Y_expanded.shape[0] == X_sliding.shape[0], \"Still mismatched!\"\n# Debugging Output\nprint(f\"Total Chunks Expected: {sum(num_chunks_per_sequence)}\")\nprint(f\"X_sliding Shape: {X_sliding.shape[0]}\")\nprint(f\"Y Shape: {Y.shape[0]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T19:33:03.085309Z","iopub.execute_input":"2025-03-28T19:33:03.085774Z","iopub.status.idle":"2025-03-28T19:33:50.338355Z","shell.execute_reply.started":"2025-03-28T19:33:03.085736Z","shell.execute_reply":"2025-03-28T19:33:50.337391Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train-test split\nX_train, X_test, Y_train, Y_test = train_test_split(X_sliding, Y_expanded, test_size=0.2, random_state=42)\nprint(f\"Training Data Shape: {X_train.shape}, Labels: {Y_train.shape}\")\nprint(f\"Testing Data Shape: {X_test.shape}, Labels: {Y_test.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T19:33:50.339301Z","iopub.execute_input":"2025-03-28T19:33:50.339614Z","iopub.status.idle":"2025-03-28T19:33:53.80938Z","shell.execute_reply.started":"2025-03-28T19:33:50.339589Z","shell.execute_reply":"2025-03-28T19:33:53.808152Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"del sequences\ndel tokenizer\ndel texts\ndel df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T19:33:53.810708Z","iopub.execute_input":"2025-03-28T19:33:53.811115Z","iopub.status.idle":"2025-03-28T19:33:56.724887Z","shell.execute_reply.started":"2025-03-28T19:33:53.811076Z","shell.execute_reply":"2025-03-28T19:33:56.718681Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc  # Garbage collector\n\n# Delete unnecessary large variables\ndel X_sliding, num_chunks_per_sequence # Large lists/arrays\n\n# Run garbage collection\ngc.collect()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T19:33:56.730498Z","iopub.execute_input":"2025-03-28T19:33:56.732036Z","iopub.status.idle":"2025-03-28T19:33:57.188819Z","shell.execute_reply.started":"2025-03-28T19:33:56.731958Z","shell.execute_reply":"2025-03-28T19:33:57.187713Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\n# 5️⃣ Load the best model\nmodel = tf.keras.models.load_model(\"/kaggle/input/bigruhatedetection/keras/default/1/biGRU.keras\")\n\n# 6️⃣ Evaluate the model\ntest_loss, test_acc = model.evaluate(X_test, Y_test)\nprint(f\"Test Accuracy: {test_acc:.4f}, Test Loss: {test_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T19:36:45.571635Z","iopub.execute_input":"2025-03-28T19:36:45.572291Z","iopub.status.idle":"2025-03-28T20:25:32.798006Z","shell.execute_reply.started":"2025-03-28T19:36:45.572243Z","shell.execute_reply":"2025-03-28T20:25:32.796398Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nprint(\"Label Counts in Y_test:\", np.sum(Y_test, axis=0))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T21:55:23.355218Z","iopub.execute_input":"2025-03-28T21:55:23.355639Z","iopub.status.idle":"2025-03-28T21:55:23.387637Z","shell.execute_reply.started":"2025-03-28T21:55:23.3556Z","shell.execute_reply":"2025-03-28T21:55:23.386442Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n# Get model predictions (probabilities)\ny_pred_probs = model.predict(X_test)\n\n# Convert probabilities to binary labels (0 or 1) using threshold = 0.5\ny_pred = (y_pred_probs >= 0.5).astype(int)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T20:25:32.800341Z","iopub.execute_input":"2025-03-28T20:25:32.800746Z","iopub.status.idle":"2025-03-28T21:14:15.731261Z","shell.execute_reply.started":"2025-03-28T20:25:32.800709Z","shell.execute_reply":"2025-03-28T21:14:15.730074Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for idx, label in enumerate(labels):\n    print(f\"{label}: True = {np.sum(Y_test[:, idx])}, Predicted = {np.sum(y_pred[:, idx])}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T21:57:04.333972Z","iopub.execute_input":"2025-03-28T21:57:04.334319Z","iopub.status.idle":"2025-03-28T21:57:04.399234Z","shell.execute_reply.started":"2025-03-28T21:57:04.334292Z","shell.execute_reply":"2025-03-28T21:57:04.397918Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\n\nprint(classification_report(Y_test, y_pred, target_names=labels, zero_division=0))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T21:57:51.841013Z","iopub.execute_input":"2025-03-28T21:57:51.841423Z","iopub.status.idle":"2025-03-28T21:57:54.779784Z","shell.execute_reply.started":"2025-03-28T21:57:51.841391Z","shell.execute_reply":"2025-03-28T21:57:54.77849Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_all_zero_labels = np.sum(np.all(Y_test == 0, axis=1))\nprint(f\"Number of samples in Y_test with no toxic labels: {num_all_zero_labels}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T21:58:24.760976Z","iopub.execute_input":"2025-03-28T21:58:24.761427Z","iopub.status.idle":"2025-03-28T21:58:24.803681Z","shell.execute_reply.started":"2025-03-28T21:58:24.761391Z","shell.execute_reply":"2025-03-28T21:58:24.802518Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_all_zero_predictions = np.sum(np.all(y_pred == 0, axis=1))\nprint(f\"Number of samples where the model predicted no toxic labels: {num_all_zero_predictions}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T21:58:58.863325Z","iopub.execute_input":"2025-03-28T21:58:58.863733Z","iopub.status.idle":"2025-03-28T21:58:58.90803Z","shell.execute_reply.started":"2025-03-28T21:58:58.863698Z","shell.execute_reply":"2025-03-28T21:58:58.907066Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Predicted 'identity_hate' counts:\", np.sum(y_pred[:, 5]))\nprint(\"Actual 'identity_hate' counts:\", np.sum(Y_test[:, 5]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T22:03:14.661987Z","iopub.execute_input":"2025-03-28T22:03:14.662482Z","iopub.status.idle":"2025-03-28T22:03:14.680786Z","shell.execute_reply.started":"2025-03-28T22:03:14.66243Z","shell.execute_reply":"2025-03-28T22:03:14.679418Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\nlabel_counts = np.sum(Y_test, axis=0)  # Count occurrences for each label\nfor i, label in enumerate(labels):\n    print(f\"Number of samples for {label}: {label_counts[i]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T22:12:55.009174Z","iopub.execute_input":"2025-03-28T22:12:55.009608Z","iopub.status.idle":"2025-03-28T22:12:55.042344Z","shell.execute_reply.started":"2025-03-28T22:12:55.009573Z","shell.execute_reply":"2025-03-28T22:12:55.04094Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\nfor i, label in enumerate(labels):\n    acc = accuracy_score(Y_test[:, i], y_pred[:, i])  # Calculate accuracy per label\n    print(f\"Accuracy for {label}: {acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T22:13:07.623125Z","iopub.execute_input":"2025-03-28T22:13:07.62349Z","iopub.status.idle":"2025-03-28T22:13:08.422663Z","shell.execute_reply.started":"2025-03-28T22:13:07.623443Z","shell.execute_reply":"2025-03-28T22:13:08.421518Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import multilabel_confusion_matrix\n\n# Compute confusion matrices for each label\ncm = multilabel_confusion_matrix(Y_test, y_pred)\n\n# Define label names\nlabels = [\"toxic\", \"severe_toxic\", \"obscene\", \"insult\", \"threat\", \"identity_hate\"]\n\n# Plot all confusion matrices\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))  # 2 rows, 3 columns\naxes = axes.ravel()\n\nfor i in range(6):\n    sns.heatmap(cm[i], annot=True, fmt='d', cmap=\"Blues\", ax=axes[i])\n    axes[i].set_title(f\"Confusion Matrix for {labels[i]}\")\n    axes[i].set_xlabel(\"Predicted Labels\")\n    axes[i].set_ylabel(\"True Labels\")\n    axes[i].set_xticklabels([\"Not Present\", \"Present\"])\n    axes[i].set_yticklabels([\"Not Present\", \"Present\"])\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T22:05:43.73212Z","iopub.execute_input":"2025-03-28T22:05:43.732685Z","iopub.status.idle":"2025-03-28T22:05:46.822737Z","shell.execute_reply.started":"2025-03-28T22:05:43.732637Z","shell.execute_reply":"2025-03-28T22:05:46.821557Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\nauc_scores = roc_auc_score(Y_test, y_pred_probs, average=None)\nfor i, label in enumerate(labels):\n    print(f\"ROC-AUC for {label}: {auc_scores[i]:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T22:10:42.999265Z","iopub.execute_input":"2025-03-28T22:10:42.99971Z","iopub.status.idle":"2025-03-28T22:10:45.02418Z","shell.execute_reply.started":"2025-03-28T22:10:42.999673Z","shell.execute_reply":"2025-03-28T22:10:45.022877Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 7))\n\nfor i, label in enumerate(labels):\n    fpr, tpr, _ = roc_curve(Y_test[:, i], y_pred_probs[:, i])  # Get FPR & TPR\n    roc_auc = auc(fpr, tpr)  # Compute AUC\n    plt.plot(fpr, tpr, label=f\"{label} (AUC = {roc_auc:.4f})\")  # Plot ROC Curve\n\n# Plot settings\nplt.plot([0, 1], [0, 1], color=\"grey\", linestyle=\"--\")  # Diagonal line\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curves for All Labels\")\nplt.legend(loc=\"lower right\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T22:10:49.018553Z","iopub.execute_input":"2025-03-28T22:10:49.018926Z","iopub.status.idle":"2025-03-28T22:10:50.911877Z","shell.execute_reply.started":"2025-03-28T22:10:49.018892Z","shell.execute_reply":"2025-03-28T22:10:50.910827Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\nroc_auc = roc_auc_score(Y_test, y_pred_probs, average=\"macro\")  # Use probs, not binary labels\nprint(f\"ROC-AUC Score: {roc_auc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T21:14:20.273015Z","iopub.execute_input":"2025-03-28T21:14:20.273424Z","iopub.status.idle":"2025-03-28T21:14:22.33096Z","shell.execute_reply.started":"2025-03-28T21:14:20.273386Z","shell.execute_reply":"2025-03-28T21:14:22.329808Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save test predictions (probabilities)\nnp.save(\"bigru_test_preds.npy\", y_pred_probs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T22:16:21.605408Z","iopub.execute_input":"2025-03-28T22:16:21.605834Z","iopub.status.idle":"2025-03-28T22:16:21.643162Z","shell.execute_reply.started":"2025-03-28T22:16:21.605802Z","shell.execute_reply":"2025-03-28T22:16:21.642207Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save training predictions (needed for stacking)\ny_train_probs = model.predict(X_train)\nnp.save(\"bigru_train_preds.npy\", y_train_probs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T22:18:23.341433Z","iopub.execute_input":"2025-03-28T22:18:23.341853Z","iopub.status.idle":"2025-03-29T01:36:46.318605Z","shell.execute_reply.started":"2025-03-28T22:18:23.341821Z","shell.execute_reply":"2025-03-29T01:36:46.316989Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save train & test labels\nnp.save(\"BiGRuY_train.npy\", Y_train)\nnp.save(\"BiGRuY_test.npy\", Y_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T01:36:46.327512Z","iopub.execute_input":"2025-03-29T01:36:46.327922Z","iopub.status.idle":"2025-03-29T01:36:47.704431Z","shell.execute_reply.started":"2025-03-29T01:36:46.327886Z","shell.execute_reply":"2025-03-29T01:36:47.702981Z"}},"outputs":[],"execution_count":null}]}