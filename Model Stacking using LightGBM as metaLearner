{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11207075,"sourceType":"datasetVersion","datasetId":6997743}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mirnasherif/meta-learner-lightgbm?scriptVersionId=230447705\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-29T14:19:32.156585Z","iopub.execute_input":"2025-03-29T14:19:32.156812Z","iopub.status.idle":"2025-03-29T14:19:33.110113Z","shell.execute_reply.started":"2025-03-29T14:19:32.156792Z","shell.execute_reply":"2025-03-29T14:19:33.109176Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n# Load saved predictions from BiGRU and BiLSTM\nbigru_train_preds = np.load(\"/kaggle/input/training-stacked-model/bigru_train_preds.npy\")  # (num_samples, num_labels)\nbilstm_train_preds = np.load(\"/kaggle/input/training-stacked-model/biLSTM_train_preds.npy\")  # (num_samples, num_labels)\n\n# Stack predictions for meta-model\nX_meta_train = np.hstack((bigru_train_preds, bilstm_train_preds))  # (num_samples, num_labels * 2)\n\n# Load true multilabel targets (Only one set!)\ny_meta_train = np.load(\"/kaggle/input/training-stacked-model/BiLSTM_Y_train.npy\")  # or use BiGRuY_train.npy\n\nprint(\"X_meta_train shape:\", X_meta_train.shape)  # Should be (num_samples, num_labels * 2)\nprint(\"y_meta_train shape:\", y_meta_train.shape)  # Should be (num_samples, num_labels)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T16:50:30.42691Z","iopub.execute_input":"2025-03-29T16:50:30.427293Z","iopub.status.idle":"2025-03-29T16:50:35.221547Z","shell.execute_reply.started":"2025-03-29T16:50:30.427255Z","shell.execute_reply":"2025-03-29T16:50:35.220305Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score\nimport joblib\n\nnum_labels = y_meta_train.shape[1]  # Number of labels\nmeta_models = {}\n\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n\nlgb_params = {\n    #\"device\": \"gpu\",  \n    #\"gpu_platform_id\": 0,\n    #\"gpu_device_id\": 0,\n    \"objective\": \"binary\",\n    \"metric\": \"binary_logloss\",\n    \"boosting_type\": \"gbdt\",\n    \"learning_rate\": 0.05,\n    \"num_leaves\": 31,\n    \"max_depth\": -1,\n    \"n_estimators\": 1000,\n    \"verbose\": -1,\n    \"is_unbalance\": True,  # ✅ Fixed\n    \"min_child_samples\": 5,\n    \"min_data_in_leaf\": 5,\n    \"max_bin\": 255,\n    \"feature_fraction\": 0.8,\n    \"bagging_fraction\": 0.8,\n    \"bagging_freq\": 5,\n    \"lambda_l1\": 0.1,\n    \"lambda_l2\": 0.1\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T16:51:52.177568Z","iopub.execute_input":"2025-03-29T16:51:52.177899Z","iopub.status.idle":"2025-03-29T16:51:52.184465Z","shell.execute_reply.started":"2025-03-29T16:51:52.177874Z","shell.execute_reply":"2025-03-29T16:51:52.183213Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from lightgbm import early_stopping, log_evaluation\n\n# Train one LightGBM model per label\nfor i in range(num_labels):\n    print(f\"Training LightGBM for label {i+1}/{num_labels}...\")\n\n    meta_preds = np.zeros(y_meta_train[:, i].shape)\n\n    for fold, (train_idx, val_idx) in enumerate(kf.split(X_meta_train, y_meta_train[:, i])):  # Fix: Stratify using label i\n        X_train, X_val = X_meta_train[train_idx], X_meta_train[val_idx]\n        y_train, y_val = y_meta_train[train_idx, i], y_meta_train[val_idx, i]\n\n        if i == 1:  # Check for the second label\n            print(f\"  Label 2, Fold {fold+1}: Unique values in y_train: {np.unique(y_train)}\")\n            print(f\"  Label 2, Fold {fold+1}: Counts of unique values in y_train: {np.unique(y_train, return_counts=True)}\")\n\n        model = lgb.LGBMClassifier(**lgb_params)\n\n        try:\n            model.fit(\n                X_train, y_train,\n                eval_set=[(X_val, y_val)],\n                eval_metric=\"binary_logloss\",\n                callbacks=[\n                    early_stopping(50),\n                    log_evaluation(10)\n                ]\n            )\n\n            # Predict on validation fold\n            meta_preds[val_idx] = model.predict_proba(X_val)[:, 1]\n\n        except lgb.LGBMError as e:\n            print(f\"Error encountered during training for Label {i+1}, Fold {fold+1}: {e}\")\n            # Optionally, you can add a break here to stop further training if the issue persists\n\n    # Evaluate\n    f1 = f1_score(y_meta_train[:, i], (meta_preds > 0.5).astype(int), average=\"macro\")\n    print(f\"Label {i+1} F1 Score: {f1:.4f}\")\n\n    # Save the trained model\n    meta_models[f\"label_{i}\"] = model\n    joblib.dump(model, f\"lightgbm_meta_label_{i}.pkl\")\n\nprint(\"✅ All meta-models trained and saved!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T16:51:56.121767Z","iopub.execute_input":"2025-03-29T16:51:56.122204Z","iopub.status.idle":"2025-03-29T18:36:26.745324Z","shell.execute_reply.started":"2025-03-29T16:51:56.122164Z","shell.execute_reply":"2025-03-29T18:36:26.744261Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\nprint(f\"Unique values in y_train for label 2: {np.unique(y_train)}\")\nprint(f\"Counts of unique values in y_train for label 2: {np.unique(y_train, return_counts=True)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T16:26:16.969959Z","iopub.execute_input":"2025-03-29T16:26:16.970253Z","iopub.status.idle":"2025-03-29T16:26:17.002112Z","shell.execute_reply.started":"2025-03-29T16:26:16.970232Z","shell.execute_reply":"2025-03-29T16:26:17.001369Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load test set predictions from base models\nbigru_test_preds = np.load(\"/kaggle/input/training-stacked-model/bigru_test_preds.npy\")\nbilstm_test_preds = np.load(\"/kaggle/input/training-stacked-model/biLSTM_test_preds.npy\")\n\n# Stack for meta-model\nX_meta_test = np.hstack((bigru_test_preds, bilstm_test_preds))\n\n# Predict using each trained model\nfinal_predictions = np.zeros((X_meta_test.shape[0], num_labels))\n\nfor i in range(num_labels):\n    meta_model = joblib.load(f\"lightgbm_meta_label_{i}.pkl\")\n    final_predictions[:, i] = meta_model.predict_proba(X_meta_test)[:, 1]\n\n# Convert to binary labels\nfinal_labels = (final_predictions > 0.5).astype(int)\n\nprint(\"Final predictions shape:\", final_labels.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T18:36:26.746807Z","iopub.execute_input":"2025-03-29T18:36:26.747285Z","iopub.status.idle":"2025-03-29T18:39:04.254461Z","shell.execute_reply.started":"2025-03-29T18:36:26.747243Z","shell.execute_reply":"2025-03-29T18:39:04.25314Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n# Try loading from the same directory as your test predictions\ny_test_path1 =np.load( \"/kaggle/input/training-stacked-model/BiGRuY_test.npy\")  # Change if needed\ny_test_path2=np.load('/kaggle/input/training-stacked-model/BiLSTM_Y_test.npy')\n\ntry:\n    y_test = y_test_path2\n    print(\"Loaded y_test with shape:\", y_test.shape)\nexcept FileNotFoundError:\n    print(\"y_test.npy not found! Make sure you saved it during test set processing.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T18:45:38.56556Z","iopub.execute_input":"2025-03-29T18:45:38.565886Z","iopub.status.idle":"2025-03-29T18:45:38.622912Z","shell.execute_reply.started":"2025-03-29T18:45:38.56586Z","shell.execute_reply":"2025-03-29T18:45:38.621665Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report, roc_auc_score, accuracy_score, confusion_matrix\n\n# Assuming you have y_test (true labels) for evaluation\nprint(\"Classification Report:\\n\", classification_report(y_test, final_labels, target_names=[f\"Label_{i}\" for i in range(num_labels)]))\n\n# Compute ROC-AUC per label\nroc_auc_scores = []\nfor i in range(num_labels):\n    if len(set(y_test[:, i])) > 1:  # Ensure at least two classes exist\n        auc = roc_auc_score(y_test[:, i], final_predictions[:, i])\n    else:\n        auc = float(\"nan\")  # If only one class exists, AUC is undefined\n    roc_auc_scores.append(auc)\n\nprint(\"\\nROC-AUC Scores:\", roc_auc_scores)\n\n# Compute overall accuracy\naccuracy = accuracy_score(y_test, final_labels)\nprint(\"\\nOverall Accuracy:\", accuracy)\n\n# Compute confusion matrices\nfor i in range(num_labels):\n    print(f\"\\nConfusion Matrix for Label {i}:\\n\", confusion_matrix(y_test[:, i], final_labels[:, i]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T18:45:41.060217Z","iopub.execute_input":"2025-03-29T18:45:41.060558Z","iopub.status.idle":"2025-03-29T18:45:49.474398Z","shell.execute_reply.started":"2025-03-29T18:45:41.06053Z","shell.execute_reply":"2025-03-29T18:45:49.473284Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve, auc\n\n\nnum_labels = y_test.shape[1]\nlabel_names = [f\"Label_{i}\" for i in range(num_labels)]\n\n# Plot Confusion Matrices\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\naxes = axes.ravel()\n\nfor i in range(num_labels):\n    cm = confusion_matrix(y_test[:, i], final_labels[:, i])\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=axes[i])\n    axes[i].set_title(f\"Confusion Matrix - {label_names[i]}\")\n    axes[i].set_xlabel(\"Predicted Label\")\n    axes[i].set_ylabel(\"True Label\")\n\nplt.tight_layout()\nplt.show()\n\n# Precision-Recall and ROC Curves\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\naxes = axes.ravel()\n\nfor i in range(num_labels):\n    precision, recall, _ = precision_recall_curve(y_test[:, i], final_predictions[:, i])\n    fpr, tpr, _ = roc_curve(y_test[:, i], final_predictions[:, i])\n    roc_auc = auc(fpr, tpr)\n\n    # PR Curve\n    axes[i].plot(recall, precision, label=f\"PR Curve (AUC={auc(recall, precision):.2f})\")\n    axes[i].set_xlabel(\"Recall\")\n    axes[i].set_ylabel(\"Precision\")\n    axes[i].set_title(f\"Precision-Recall Curve - {label_names[i]}\")\n    axes[i].legend()\n\nplt.tight_layout()\nplt.show()\n\n# ROC Curve\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\naxes = axes.ravel()\n\nfor i in range(num_labels):\n    fpr, tpr, _ = roc_curve(y_test[:, i], final_predictions[:, i])\n    roc_auc = auc(fpr, tpr)\n\n    axes[i].plot(fpr, tpr, label=f\"ROC Curve (AUC={roc_auc:.2f})\")\n    axes[i].plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n    axes[i].set_xlabel(\"False Positive Rate\")\n    axes[i].set_ylabel(\"True Positive Rate\")\n    axes[i].set_title(f\"ROC Curve - {label_names[i]}\")\n    axes[i].legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T18:50:22.846736Z","iopub.execute_input":"2025-03-29T18:50:22.847095Z","iopub.status.idle":"2025-03-29T18:50:42.172596Z","shell.execute_reply.started":"2025-03-29T18:50:22.847064Z","shell.execute_reply":"2025-03-29T18:50:42.171326Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\n# Load test labels and predictions\ny_test = np.load(\"/kaggle/input/training-stacked-model/BiGRuY_test.npy\")\nbigru_test_preds = np.load(\"/kaggle/input/training-stacked-model/bigru_test_preds.npy\")\nbilstm_test_preds = np.load(\"/kaggle/input/training-stacked-model/biLSTM_test_preds.npy\")\n\n# Load stacked model predictions\nX_meta_test = np.hstack((bigru_test_preds, bilstm_test_preds))\nfinal_predictions = np.zeros((X_meta_test.shape[0], y_test.shape[1]))\n\nfor i in range(y_test.shape[1]):\n    meta_model = joblib.load(f\"lightgbm_meta_label_{i}.pkl\")\n    final_predictions[:, i] = meta_model.predict_proba(X_meta_test)[:, 1]\n\n# Convert to binary labels\nfinal_labels = (final_predictions > 0.5).astype(int)\n\n# 1. Plot low-confidence predictions (0.4 < prob < 0.6)\nuncertain_mask = (final_predictions > 0.4) & (final_predictions < 0.6)\nuncertain_counts = uncertain_mask.sum(axis=0)\n\nplt.figure(figsize=(10, 5))\nsns.barplot(x=[f\"Label_{i}\" for i in range(y_test.shape[1])], y=uncertain_counts)\nplt.xticks(rotation=45)\nplt.title(\"Number of Low-Confidence Predictions per Label (0.4 < p < 0.6)\")\nplt.ylabel(\"Count\")\nplt.show()\n\n# 2. Compute misclassifications\nfalse_positives = np.logical_and(final_labels == 1, y_test == 0).sum(axis=0)\nfalse_negatives = np.logical_and(final_labels == 0, y_test == 1).sum(axis=0)\n\n# Plot False Positives & False Negatives per Label\nplt.figure(figsize=(12, 5))\nlabels = [f\"Label_{i}\" for i in range(y_test.shape[1])]\nx = np.arange(len(labels))\n\nplt.bar(x - 0.2, false_positives, width=0.4, label=\"False Positives\", color='r')\nplt.bar(x + 0.2, false_negatives, width=0.4, label=\"False Negatives\", color='b')\n\nplt.xticks(x, labels, rotation=45)\nplt.title(\"False Positives & False Negatives per Label\")\nplt.ylabel(\"Count\")\nplt.legend()\nplt.show()\n\n# 3. Label correlation in errors (heatmap)\nerror_correlation = np.corrcoef(final_labels.T)\nplt.figure(figsize=(8, 6))\nsns.heatmap(error_correlation, annot=True, cmap=\"coolwarm\", xticklabels=labels, yticklabels=labels)\nplt.title(\"Correlation of Errors Between Labels\")\nplt.show()\n\n# 4. Confusion Matrices for each label\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\n\nfor i, ax in enumerate(axes.flatten()):\n    cm = confusion_matrix(y_test[:, i], final_labels[:, i])\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax)\n    ax.set_title(f\"Confusion Matrix - {labels[i]}\")\n    ax.set_xlabel(\"Predicted\")\n    ax.set_ylabel(\"Actual\")\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T18:54:56.863023Z","iopub.execute_input":"2025-03-29T18:54:56.863728Z","iopub.status.idle":"2025-03-29T18:57:40.579744Z","shell.execute_reply.started":"2025-03-29T18:54:56.863691Z","shell.execute_reply":"2025-03-29T18:57:40.578718Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.save(\"final_predictions.npy\", final_labels)\nprint(\"Final predictions saved!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T18:39:04.256134Z","iopub.execute_input":"2025-03-29T18:39:04.256547Z","iopub.status.idle":"2025-03-29T18:39:04.297523Z","shell.execute_reply.started":"2025-03-29T18:39:04.256508Z","shell.execute_reply":"2025-03-29T18:39:04.296461Z"}},"outputs":[],"execution_count":null}]}