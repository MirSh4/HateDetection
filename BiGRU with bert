{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11111498,"sourceType":"datasetVersion","datasetId":6926583},{"sourceId":11120959,"sourceType":"datasetVersion","datasetId":6934896},{"sourceId":11130665,"sourceType":"datasetVersion","datasetId":6941960}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mirnasherif/bigru?scriptVersionId=234134410\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-22T22:31:13.900362Z","iopub.execute_input":"2025-03-22T22:31:13.9007Z","iopub.status.idle":"2025-03-22T22:31:14.226886Z","shell.execute_reply.started":"2025-03-22T22:31:13.900665Z","shell.execute_reply":"2025-03-22T22:31:14.225917Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\n\n# List all available GPUs\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        # Set memory growth to avoid using all GPU memory at once\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(\"Using GPU:\", gpus)\n    except RuntimeError as e:\n        print(\"Error:\", e)\nelse:\n    print(\"No GPU detected. Running on CPU.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T21:12:12.593464Z","iopub.execute_input":"2025-04-15T21:12:12.59415Z","iopub.status.idle":"2025-04-15T21:12:12.826627Z","shell.execute_reply.started":"2025-04-15T21:12:12.594116Z","shell.execute_reply":"2025-04-15T21:12:12.825783Z"}},"outputs":[{"name":"stdout","text":"Using GPU: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Input, Embedding, LSTM, Bidirectional, Dense, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom transformers import BertTokenizer, TFBertModel\nimport tensorflow as tf\nimport numpy as np\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T21:11:49.805849Z","iopub.execute_input":"2025-04-15T21:11:49.806153Z","iopub.status.idle":"2025-04-15T21:12:12.592172Z","shell.execute_reply.started":"2025-04-15T21:11:49.806118Z","shell.execute_reply":"2025-04-15T21:12:12.591473Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Load the pre-trained BERT tokenizer and model\nbtokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nbert_model = TFBertModel.from_pretrained('bert-base-uncased')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T21:12:12.828248Z","iopub.execute_input":"2025-04-15T21:12:12.828496Z","iopub.status.idle":"2025-04-15T21:12:20.456813Z","shell.execute_reply.started":"2025-04-15T21:12:12.828475Z","shell.execute_reply":"2025-04-15T21:12:20.456128Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48462ae180624d53a94ece464efd1d0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d7801d89b2a4af490aaf0e6ccac6f28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5314cdda2f2c4b09a61f2d78ca7d537c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0aee7818f6249c5875b82892ca792bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f58fdecf91c4f14b4b23de82a1df413"}},"metadata":{}},{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Load dataset\ndf = pd.read_csv(\"/kaggle/input/jigsaw-competition-merged-and-balanced/cleaned_data.csv\")\nlabels = [\"toxic\", \"severe_toxic\", \"obscene\", \"insult\", \"threat\", \"identity_hate\"]  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T21:12:20.45757Z","iopub.execute_input":"2025-04-15T21:12:20.457813Z","iopub.status.idle":"2025-04-15T21:12:44.797738Z","shell.execute_reply.started":"2025-04-15T21:12:20.457793Z","shell.execute_reply":"2025-04-15T21:12:44.797044Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T21:12:44.799454Z","iopub.execute_input":"2025-04-15T21:12:44.799744Z","iopub.status.idle":"2025-04-15T21:12:44.823919Z","shell.execute_reply.started":"2025-04-15T21:12:44.799722Z","shell.execute_reply":"2025-04-15T21:12:44.823003Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                        comment_text  toxic  severe_toxic  \\\n0                           asshole piss around work      1             1   \n1  hey talk exclusive group wp talibans good dest...      1             0   \n2            bye look come think comming back tosser      1             0   \n3  monetary value gay manoeuver room antisemmitia...      1             0   \n4               sleep together filthy mother ass dry      1             0   \n\n   obscene  threat  insult  identity_hate  \n0        1       0       1              0  \n1        0       0       0              0  \n2        0       0       0              0  \n3        1       0       1              1  \n4        1       0       1              0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>comment_text</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>asshole piss around work</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>hey talk exclusive group wp talibans good dest...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>bye look come think comming back tosser</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>monetary value gay manoeuver room antisemmitia...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>sleep together filthy mother ass dry</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T21:12:44.825191Z","iopub.execute_input":"2025-04-15T21:12:44.825433Z","iopub.status.idle":"2025-04-15T21:12:44.852286Z","shell.execute_reply.started":"2025-04-15T21:12:44.825413Z","shell.execute_reply":"2025-04-15T21:12:44.85145Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"Index(['comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult',\n       'identity_hate'],\n      dtype='object')"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"df=df.dropna()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T21:12:44.853096Z","iopub.execute_input":"2025-04-15T21:12:44.853416Z","iopub.status.idle":"2025-04-15T21:12:45.577126Z","shell.execute_reply.started":"2025-04-15T21:12:44.853387Z","shell.execute_reply":"2025-04-15T21:12:45.576196Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import re\n\n# Function to check if a sentence contains special characters\ndef has_special_chars(text):\n    return bool(re.search(r'[^a-zA-Z0-9\\s]', text))  # Returns True if special chars exist\n\n# Filter sentences with special characters\nspecial_char_sentences = df[df['comment_text'].apply(has_special_chars)]\n\n# Display them\nprint(special_char_sentences)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T21:12:45.578004Z","iopub.execute_input":"2025-04-15T21:12:45.578239Z","iopub.status.idle":"2025-04-15T21:12:56.413413Z","shell.execute_reply.started":"2025-04-15T21:12:45.578219Z","shell.execute_reply":"2025-04-15T21:12:56.412476Z"}},"outputs":[{"name":"stdout","text":"                                              comment_text  toxic  \\\n37       seen reference claim moron source it´s pov tim...      1   \n70       hebrew name lydia appologies til eulenspiegel ...      0   \n146      camille paglia sex positive feminist supportin...      1   \n150      ——————————————————————————————————————————————...      1   \n177      take attest presented tonicity evidence merely...      1   \n...                                                    ...    ...   \n4707645  freddie getting lots shots he’s letting soft g...      0   \n4707675  guy came jummah started spewing hate violent w...      0   \n4707719  nobody thought invite yazidi women tell honora...      0   \n4707786  good grief addressed tim mueller’s comment ask...      0   \n4707797  know ignorant repressed would turn comment aro...      0   \n\n         severe_toxic  obscene  threat  insult  identity_hate  \n37                  0        0       0       0              0  \n70                  0        0       0       1              0  \n146                 0        0       0       0              0  \n150                 0        1       0       1              0  \n177                 0        1       0       1              1  \n...               ...      ...     ...     ...            ...  \n4707645             0        0       0       0              0  \n4707675             0        0       0       0              0  \n4707719             0        0       0       0              0  \n4707786             0        0       0       0              0  \n4707797             0        0       0       0              0  \n\n[176321 rows x 7 columns]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import re\n\n# Function to remove special characters\ndef remove_special_chars(text):\n    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Keeps only letters, numbers, and spaces\n\n# Apply to your dataframe\ndf['comment_text'] = df['comment_text'].astype(str).apply(remove_special_chars)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T21:12:56.41432Z","iopub.execute_input":"2025-04-15T21:12:56.414652Z","iopub.status.idle":"2025-04-15T21:13:08.582051Z","shell.execute_reply.started":"2025-04-15T21:12:56.414618Z","shell.execute_reply":"2025-04-15T21:13:08.581096Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import pickle\n\n# Load the tokenizer\nwith open('/kaggle/input/hatedetectiontokenizer/tokenizer.pkl', 'rb') as f:\n    tokenizer = pickle.load(f)\n\n# Confirm tokenizer loaded correctly\nprint(f\"Tokenizer vocabulary size: {len(tokenizer.word_index)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T21:13:08.583122Z","iopub.execute_input":"2025-04-15T21:13:08.583447Z","iopub.status.idle":"2025-04-15T21:13:09.594506Z","shell.execute_reply.started":"2025-04-15T21:13:08.583418Z","shell.execute_reply":"2025-04-15T21:13:09.593749Z"}},"outputs":[{"name":"stdout","text":"Tokenizer vocabulary size: 478020\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import numpy as np\n\n# Load the precomputed embedding matrix\nembedding_matrix = np.load('/kaggle/input/embeddingmatrix/embedding_matrix.npy')\n\n# Check the shape\nprint(f\"Embedding Matrix Shape: {embedding_matrix.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T21:13:09.595263Z","iopub.execute_input":"2025-04-15T21:13:09.595488Z","iopub.status.idle":"2025-04-15T21:13:15.816249Z","shell.execute_reply.started":"2025-04-15T21:13:09.59547Z","shell.execute_reply":"2025-04-15T21:13:15.81544Z"}},"outputs":[{"name":"stdout","text":"Embedding Matrix Shape: (478019, 300)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T21:13:15.816977Z","iopub.execute_input":"2025-04-15T21:13:15.817271Z","iopub.status.idle":"2025-04-15T21:13:15.821085Z","shell.execute_reply.started":"2025-04-15T21:13:15.817239Z","shell.execute_reply":"2025-04-15T21:13:15.820339Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"vocab_size=478019\nprint(f\"Vocabulary size: {vocab_size}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T21:13:15.821826Z","iopub.execute_input":"2025-04-15T21:13:15.822052Z","iopub.status.idle":"2025-04-15T21:13:15.837782Z","shell.execute_reply.started":"2025-04-15T21:13:15.822021Z","shell.execute_reply":"2025-04-15T21:13:15.836938Z"}},"outputs":[{"name":"stdout","text":"Vocabulary size: 478019\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"fit_on_texts(texts): Learns word mappings and counts vocabulary size.\ntexts_to_sequences(texts): Converts each text into a sequence of numbers.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nMAX_WORDS = vocab_size\nMAX_SEQUENCE_LENGTH = 100  # Use 95th percentile length :86\nSTRIDE =50   # Fix stride calculation\nEMBEDDING_DIM = 300  # Keep the same unless changing embeddings\n\n\ntexts = df[\"comment_text\"].astype(str).values  # Convert comments to string\nsequences = tokenizer.texts_to_sequences(texts)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T21:13:15.839864Z","iopub.execute_input":"2025-04-15T21:13:15.840091Z","iopub.status.idle":"2025-04-15T21:14:55.524685Z","shell.execute_reply.started":"2025-04-15T21:13:15.840073Z","shell.execute_reply":"2025-04-15T21:14:55.523754Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def get_bert_embeddings_with_sliding_window(texts, max_length=100, stride=50):\n    all_embeddings = []\n\n    for text in texts:\n        # Tokenize full text (to get total tokens)\n        tokens = btokenizer(text, return_tensors='tf', truncation=False)['input_ids'][0]\n\n        # Store pooled embeddings for all windows\n        pooled_outputs = []\n\n        for i in range(0, len(tokens), stride):\n            window_tokens = tokens[i:i + max_length]\n\n            if len(window_tokens) == 0:\n                continue\n\n            window_tokens = tf.pad(window_tokens, [[0, max(0, max_length - tf.shape(window_tokens)[0])]])\n\n            input_dict = {\n                'input_ids': tf.expand_dims(window_tokens, 0),\n                'attention_mask': tf.expand_dims(tf.cast(window_tokens != btokenizer.pad_token_id, tf.int32), 0)\n            }\n\n            # Get BERT outputs\n            outputs = bert_model(input_dict)\n            hidden_states = outputs.last_hidden_state  # (1, seq_len, hidden_dim)\n\n            # Mean pool over non-padding tokens\n            attention_mask = input_dict['attention_mask']\n            masked_embeddings = hidden_states * tf.cast(tf.expand_dims(attention_mask, -1), tf.float32)\n            sum_embeddings = tf.reduce_sum(masked_embeddings, axis=1)\n            counts = tf.reduce_sum(tf.cast(attention_mask, tf.float32), axis=1, keepdims=True)\n            pooled = sum_embeddings / counts\n\n            pooled_outputs.append(pooled)\n\n        # Aggregate all windows for this text\n        if pooled_outputs:\n            final_embedding = tf.reduce_mean(tf.concat(pooled_outputs, axis=0), axis=0)\n        else:\n            final_embedding = tf.zeros([bert_model.config.hidden_size])\n\n        all_embeddings.append(final_embedding)\n\n    return tf.stack(all_embeddings).numpy()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T21:15:00.694358Z","iopub.execute_input":"2025-04-15T21:15:00.694721Z","iopub.status.idle":"2025-04-15T21:15:00.701976Z","shell.execute_reply.started":"2025-04-15T21:15:00.694693Z","shell.execute_reply":"2025-04-15T21:15:00.700991Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Sliding Window Function\ndef create_sliding_window_sequences(sequences, max_len=MAX_SEQUENCE_LENGTH, stride=50):\n    all_chunks = []\n    for seq in sequences:\n        chunks = []\n        if len(seq) <= max_len:\n            seq += [0] * (max_len - len(seq))\n            chunks.append(seq)\n        else:\n            for i in range(0, len(seq) - max_len + 1, stride):\n                chunks.append(seq[i : i + max_len])\n            if (len(seq) - max_len) % stride != 0:\n                chunks.append(seq[-max_len:])\n        all_chunks.extend(chunks)\n    return np.array(all_chunks)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T21:15:04.300961Z","iopub.execute_input":"2025-04-15T21:15:04.301285Z","iopub.status.idle":"2025-04-15T21:15:04.306359Z","shell.execute_reply.started":"2025-04-15T21:15:04.301257Z","shell.execute_reply":"2025-04-15T21:15:04.305497Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Apply sliding window\nX_sliding = create_sliding_window_sequences(sequences)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T21:15:08.439061Z","iopub.execute_input":"2025-04-15T21:15:08.439384Z","iopub.status.idle":"2025-04-15T21:15:56.320476Z","shell.execute_reply.started":"2025-04-15T21:15:08.439361Z","shell.execute_reply":"2025-04-15T21:15:56.319754Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Ensure Y is a 1D array\nY = df[labels].values  # Shape: (4703664, 6)\n\n# Count how many chunks per original sequence\nnum_chunks_per_sequence = [len(create_sliding_window_sequences([seq])) for seq in sequences]\n\n# Repeat labels correctly\nY_expanded = np.repeat(Y, num_chunks_per_sequence, axis=0)\n\n# Verify shapes again\nprint(f\"Fixed Shapes - X_sliding: {X_sliding.shape}, Y_expanded: {Y_expanded.shape}\")\nassert Y_expanded.shape[0] == X_sliding.shape[0], \"Still mismatched!\"\n# Debugging Output\nprint(f\"Total Chunks Expected: {sum(num_chunks_per_sequence)}\")\nprint(f\"X_sliding Shape: {X_sliding.shape[0]}\")\nprint(f\"Y Shape: {Y.shape[0]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T21:15:56.331734Z","iopub.execute_input":"2025-04-15T21:15:56.331985Z","iopub.status.idle":"2025-04-15T21:16:35.528103Z","shell.execute_reply.started":"2025-04-15T21:15:56.331964Z","shell.execute_reply":"2025-04-15T21:16:35.527318Z"}},"outputs":[{"name":"stdout","text":"Fixed Shapes - X_sliding: (5396017, 100), Y_expanded: (5396017, 6)\nTotal Chunks Expected: 5396017\nX_sliding Shape: 5396017\nY Shape: 4703664\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Assuming Y_expanded contains the multilabels (0 or 1 values for each label)\n# If not, ensure Y_expanded is the correct format (binary labels for each sample)\n\nprint(f\"Y_expanded shape: {Y_expanded.shape}\")\nprint(f\"Y_expanded sample: {Y_expanded[0]}\")  # Check the first row (sample)\n\n# Calculate class frequencies: sum across all samples for each label (axis=0 sums the columns)\nclass_frequencies = np.sum(Y_expanded, axis=0)  # For multilabel, we sum along axis=0 (the samples)\n\n# Calculate the inverse frequency (rare classes get higher weight)\nalpha = 1.0 / (class_frequencies + 1e-6)  # Adding a small epsilon to avoid division by zero\n\n# Normalize alpha so it sums to 1 (optional, but keeps things balanced)\nalpha = alpha / np.sum(alpha)\n\nprint(\"Alpha values for each label:\", alpha)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T21:16:35.529406Z","iopub.execute_input":"2025-04-15T21:16:35.529662Z","iopub.status.idle":"2025-04-15T21:16:35.631745Z","shell.execute_reply.started":"2025-04-15T21:16:35.529642Z","shell.execute_reply":"2025-04-15T21:16:35.630828Z"}},"outputs":[{"name":"stdout","text":"Y_expanded shape: (5396017, 6)\nY_expanded sample: [1 1 1 1 0 0]\nAlpha values for each label: [0.06584052 0.17696853 0.11540265 0.10581925 0.30466007 0.23130898]\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# Train-test split\nX_train, X_test, Y_train, Y_test = train_test_split(X_sliding, Y_expanded, test_size=0.2, random_state=42)\nprint(f\"Training Data Shape: {X_train.shape}, Labels: {Y_train.shape}\")\nprint(f\"Testing Data Shape: {X_test.shape}, Labels: {Y_test.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T21:16:35.632948Z","iopub.execute_input":"2025-04-15T21:16:35.633231Z","iopub.status.idle":"2025-04-15T21:16:38.317414Z","shell.execute_reply.started":"2025-04-15T21:16:35.633209Z","shell.execute_reply":"2025-04-15T21:16:38.31667Z"}},"outputs":[{"name":"stdout","text":"Training Data Shape: (4316813, 100), Labels: (4316813, 6)\nTesting Data Shape: (1079204, 100), Labels: (1079204, 6)\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"def focal_loss(gamma=2., alpha=None):\n    \"\"\"\n    Focal Loss for multilabel classification with per-class alpha.\n    \n    gamma: focusing parameter (default: 2)\n    alpha: class weight (default: None, should be passed if balancing needed)\n    \"\"\"\n    def focal_loss_fixed(y_true, y_pred):\n        # Clip predictions to prevent log(0)\n        y_pred = K.clip(y_pred, K.epsilon(), 1. - K.epsilon())\n        \n        # Cross entropy\n        cross_entropy = -y_true * K.log(y_pred)\n        \n        # If alpha is provided, apply it per class\n        if alpha is not None:\n            alpha_t = alpha\n        else:\n            alpha_t = 1.0\n        \n        # Focal loss formula\n        loss = alpha_t * K.pow(1 - y_pred, gamma) * cross_entropy\n        \n        # Return the mean loss\n        return K.mean(K.sum(loss, axis=1))\n    \n    return focal_loss_fixed\n\nimport tensorflow as tf\nfrom tensorflow.keras import backend as K\n\ndef focal_loss_multilabel(alpha, gamma=2.0):\n    alpha = tf.constant(alpha, dtype=tf.float32)\n    \n    def loss(y_true, y_pred):\n        y_pred = tf.clip_by_value(y_pred, K.epsilon(), 1.0 - K.epsilon())\n        \n        cross_entropy = -y_true * tf.math.log(y_pred) - (1 - y_true) * tf.math.log(1 - y_pred)\n        weight = y_true * alpha * tf.pow(1 - y_pred, gamma) + (1 - y_true) * (1 - alpha) * tf.pow(y_pred, gamma)\n        \n        loss = weight * cross_entropy\n        return tf.reduce_mean(tf.reduce_sum(loss, axis=1))  # Sum over classes, mean over batch\n\n    return loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T21:16:38.318184Z","iopub.execute_input":"2025-04-15T21:16:38.318396Z","iopub.status.idle":"2025-04-15T21:16:38.341078Z","shell.execute_reply.started":"2025-04-15T21:16:38.318377Z","shell.execute_reply":"2025-04-15T21:16:38.340031Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Embedding, GRU, Bidirectional, Dense, Dropout\nfrom tensorflow.keras.models import Model\n\ndef BiGRU():\n    # Text Input\n    input_text = Input(shape=(MAX_SEQUENCE_LENGTH,))\n\n    # Embedding Layer (Make sure embedding_matrix is loaded)\n    embedding_layer = Embedding(\n        input_dim=MAX_WORDS,\n        output_dim=EMBEDDING_DIM,\n        weights=[embedding_matrix],  # Preloaded GloVe/FastText embeddings\n        trainable=False\n    )(input_text)\n\n    # BiLSTM Model\n    x = Bidirectional(LSTM(128, return_sequences=True))(embedding_layer)\n    x = GRU(64)(x)\n\n    # Fully Connected Layers\n    x = Dense(128, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    \n    # Output Layer (Multilabel classification)\n    output = Dense(len(labels), activation='sigmoid')(x)\n\n    # Create & Compile Model\n    model = Model(inputs=input_text, outputs=output)\n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T21:16:38.342009Z","iopub.execute_input":"2025-04-15T21:16:38.342324Z","iopub.status.idle":"2025-04-15T21:16:38.360205Z","shell.execute_reply.started":"2025-04-15T21:16:38.342294Z","shell.execute_reply":"2025-04-15T21:16:38.359337Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, GRU, Dense, Dropout, Concatenate\nfrom tensorflow.keras.models import Model\n\ndef BiGRU_with_BERT():\n    # BiGRU Text Input\n    input_text = Input(shape=(MAX_SEQUENCE_LENGTH,))\n    \n    # Embedding Layer (GloVe/FastText or pre-trained embedding_matrix)\n    embedding_layer = Embedding(\n        input_dim=MAX_WORDS,\n        output_dim=EMBEDDING_DIM,\n        weights=[embedding_matrix],  # Preloaded GloVe/FastText embeddings\n        trainable=False\n    )(input_text)\n    \n    # BiLSTM Model Layer\n    x = Bidirectional(LSTM(128, return_sequences=True))(embedding_layer)\n    x = GRU(64)(x)\n\n    # BERT Embeddings Input\n    input_bert = Input(shape=(EMBEDDING_DIM,))  # BERT embeddings will have the same shape as the BiGRU embeddings\n\n    # Concatenate the BiGRU and BERT embeddings\n    combined = Concatenate()([x, input_bert])\n\n    # Fully Connected Layers\n    x = Dense(128, activation='relu')(combined)\n    x = Dropout(0.3)(x)\n    \n    # Output Layer (Multilabel classification)\n    output = Dense(len(labels), activation='sigmoid')(x)\n\n    # Create & Compile Model\n    model = Model(inputs=[input_text, input_bert], outputs=output)\n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T21:16:38.361306Z","iopub.execute_input":"2025-04-15T21:16:38.36169Z","iopub.status.idle":"2025-04-15T21:16:38.383224Z","shell.execute_reply.started":"2025-04-15T21:16:38.361627Z","shell.execute_reply":"2025-04-15T21:16:38.382359Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"from tensorflow.keras.callbacks import Callback\nimport tensorflow as tf\n\ncallbacks = [\n    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2,mode=\"min\",  # Want the lowest loss\n    restore_best_weights=True,  # Load best model after stopping\n    verbose=1),\n    tf.keras.callbacks.ModelCheckpoint(\"best.keras\",save_best_only=True,monitor='val_loss', mode='min')\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T21:16:38.384972Z","iopub.execute_input":"2025-04-15T21:16:38.385244Z","iopub.status.idle":"2025-04-15T21:16:38.403509Z","shell.execute_reply.started":"2025-04-15T21:16:38.385222Z","shell.execute_reply":"2025-04-15T21:16:38.402534Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Example of getting BERT embeddings\ntexts = df[\"comment_text\"].astype(str).values  # Convert comments to string\nbert_embeddings = get_bert_embeddings_with_sliding_window(texts)\n\n# Now, concatenate the BiGRU input with BERT embeddings\nX_train_combined = [X_train, bert_embeddings[:len(X_train)]]  # Use X_train for BiGRU + BERT embeddings for BERT\nX_test_combined = [X_test, bert_embeddings[len(X_train):]]  # Similarly for X_test\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T21:17:08.971034Z","iopub.execute_input":"2025-04-15T21:17:08.971337Z"}},"outputs":[{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (1028 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"del sequences\ndel tokenizer\ndel texts\ndel df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc  # Garbage collector\n\n# Delete unnecessary large variables\ndel X_sliding, num_chunks_per_sequence # Large lists/arrays\n\n# Run garbage collection\ngc.collect()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"alpha = np.array([0.06584052, 0.17696853, 0.11540265, 0.10581925, 0.30466007, 0.23130898])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = BiGRU_with_BERT()\n#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.compile(\n    loss=focal_loss_multilabel(alpha=alpha, gamma=2.0),\n    optimizer='adam',\n    metrics=['accuracy']\n)\n\n\n# Use your previous training setup\nhistory = model.fit(\n    X_train_combined, Y_train,  # Pass both BiGRU and BERT inputs\n    epochs=15,\n    validation_split=0.1,\n    batch_size=32,\n    shuffle=True,\n    callbacks=callbacks,\n    verbose=1,\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get model predictions (probabilities)\ny_pred_probs = model.predict(X_test_combined)\n\n# Convert probabilities to binary labels (0 or 1) using threshold = 0.5\ny_pred = (y_pred_probs >= 0.5).astype(int)\n\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-22T23:20:31.341Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint(\"Classification Report:\\n\", classification_report(Y_test, y_pred, target_names=labels))\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-22T23:20:31.342Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\n\n# Generate confusion matrix\nconf_matrix = confusion_matrix(Y_test.argmax(axis=1), y_pred.argmax(axis=1))\n\n# Plot confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\nplt.xlabel(\"Predicted Labels\")\nplt.ylabel(\"True Labels\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-22T23:20:31.342Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\nroc_auc = roc_auc_score(Y_test, y_pred_probs, average=\"macro\")  # Use probs, not binary labels\nprint(f\"ROC-AUC Score: {roc_auc:.4f}\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-22T23:20:31.342Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Save the trained BiGRU model\nmodel.save(\"bigru_model.keras\")\n\n# 2. Save test predictions (probabilities)\nnp.save(\"bigru_test_preds.npy\", y_pred_probs)\n\n# 3. Save training predictions (used in stacking layer)\ny_train_probs = model.predict(X_train, batch_size=128, verbose=1)\nnp.save(\"bigru_train_preds.npy\", y_train_probs)\n\n# 4. Save Y labels\nnp.save(\"bigru_Y_train.npy\", Y_train)\nnp.save(\"bigru_Y_test.npy\", Y_test)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-22T23:20:31.342Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\nwith open(\"bigru_tokenizer.pkl\", \"wb\") as f:\n    pickle.dump(tokenizer, f)\n\n# 6. (Optional) Save class weights or alpha values for reproducibility\nnp.save(\"bigru_alpha_weights.npy\", alpha)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}