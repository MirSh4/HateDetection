{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11111498,"sourceType":"datasetVersion","datasetId":6926583},{"sourceId":11120959,"sourceType":"datasetVersion","datasetId":6934896},{"sourceId":11130665,"sourceType":"datasetVersion","datasetId":6941960},{"sourceId":306041,"sourceType":"modelInstanceVersion","modelInstanceId":261095,"modelId":282249},{"sourceId":307265,"sourceType":"modelInstanceVersion","modelInstanceId":261846,"modelId":282991}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mirnasherif/bilstm-results?scriptVersionId=230369576\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-29T14:14:19.568248Z","iopub.execute_input":"2025-03-29T14:14:19.568671Z","iopub.status.idle":"2025-03-29T14:14:19.583595Z","shell.execute_reply.started":"2025-03-29T14:14:19.568632Z","shell.execute_reply":"2025-03-29T14:14:19.582345Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Input, Embedding, LSTM, Bidirectional, Dense, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T14:14:23.025888Z","iopub.execute_input":"2025-03-29T14:14:23.026246Z","iopub.status.idle":"2025-03-29T14:14:23.032778Z","shell.execute_reply.started":"2025-03-29T14:14:23.026205Z","shell.execute_reply":"2025-03-29T14:14:23.031504Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load dataset\ndf = pd.read_csv(\"/kaggle/input/jigsaw-competition-merged-and-balanced/cleaned_data.csv\")\nlabels = [\"toxic\", \"severe_toxic\", \"obscene\", \"insult\", \"threat\", \"identity_hate\"]  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T14:14:25.562019Z","iopub.execute_input":"2025-03-29T14:14:25.562344Z","iopub.status.idle":"2025-03-29T14:14:45.431821Z","shell.execute_reply.started":"2025-03-29T14:14:25.562318Z","shell.execute_reply":"2025-03-29T14:14:45.429823Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T14:14:45.820302Z","iopub.execute_input":"2025-03-29T14:14:45.820663Z","iopub.status.idle":"2025-03-29T14:14:45.851529Z","shell.execute_reply.started":"2025-03-29T14:14:45.820622Z","shell.execute_reply":"2025-03-29T14:14:45.850356Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:05:59.560578Z","iopub.execute_input":"2025-03-29T15:05:59.561017Z","iopub.status.idle":"2025-03-29T15:05:59.569793Z","shell.execute_reply.started":"2025-03-29T15:05:59.560972Z","shell.execute_reply":"2025-03-29T15:05:59.568465Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df=df.dropna()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:05:59.613982Z","iopub.execute_input":"2025-03-29T15:05:59.614297Z","iopub.status.idle":"2025-03-29T15:06:00.607448Z","shell.execute_reply.started":"2025-03-29T15:05:59.614271Z","shell.execute_reply":"2025-03-29T15:06:00.606426Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\n\n# Function to check if a sentence contains special characters\ndef has_special_chars(text):\n    return bool(re.search(r'[^a-zA-Z0-9\\s]', text))  # Returns True if special chars exist\n\n# Filter sentences with special characters\nspecial_char_sentences = df[df['comment_text'].apply(has_special_chars)]\n\n# Display them\nprint(special_char_sentences)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:57:04.594939Z","iopub.execute_input":"2025-03-29T15:57:04.595392Z","iopub.status.idle":"2025-03-29T15:57:20.851739Z","shell.execute_reply.started":"2025-03-29T15:57:04.595351Z","shell.execute_reply":"2025-03-29T15:57:20.850559Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\n\n# Function to remove special characters\ndef remove_special_chars(text):\n    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Keeps only letters, numbers, and spaces\n\n# Apply to your dataframe\ndf['comment_text'] = df['comment_text'].astype(str).apply(remove_special_chars)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:57:20.934778Z","iopub.execute_input":"2025-03-29T15:57:20.935178Z","iopub.status.idle":"2025-03-29T15:57:35.636576Z","shell.execute_reply.started":"2025-03-29T15:57:20.935147Z","shell.execute_reply":"2025-03-29T15:57:35.635618Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\n\n# Load the tokenizer\nwith open('/kaggle/input/hatedetectiontokenizer/tokenizer.pkl', 'rb') as f:\n    tokenizer = pickle.load(f)\n\n# Confirm tokenizer loaded correctly\nprint(f\"Tokenizer vocabulary size: {len(tokenizer.word_index)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:57:38.672372Z","iopub.execute_input":"2025-03-29T15:57:38.67269Z","iopub.status.idle":"2025-03-29T15:57:39.871618Z","shell.execute_reply.started":"2025-03-29T15:57:38.672663Z","shell.execute_reply":"2025-03-29T15:57:39.87049Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n# Load the precomputed embedding matrix\nembedding_matrix = np.load('/kaggle/input/embeddingmatrix/embedding_matrix.npy')\n\n# Check the shape\nprint(f\"Embedding Matrix Shape: {embedding_matrix.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:57:39.917235Z","iopub.execute_input":"2025-03-29T15:57:39.917555Z","iopub.status.idle":"2025-03-29T15:57:47.776751Z","shell.execute_reply.started":"2025-03-29T15:57:39.917528Z","shell.execute_reply":"2025-03-29T15:57:47.775456Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:57:47.820914Z","iopub.execute_input":"2025-03-29T15:57:47.821225Z","iopub.status.idle":"2025-03-29T15:57:47.827426Z","shell.execute_reply.started":"2025-03-29T15:57:47.821199Z","shell.execute_reply":"2025-03-29T15:57:47.826205Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vocab_size=478019\nprint(f\"Vocabulary size: {vocab_size}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:57:47.860803Z","iopub.execute_input":"2025-03-29T15:57:47.861158Z","iopub.status.idle":"2025-03-29T15:57:47.866939Z","shell.execute_reply.started":"2025-03-29T15:57:47.861129Z","shell.execute_reply":"2025-03-29T15:57:47.865548Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"fit_on_texts(texts): Learns word mappings and counts vocabulary size.\ntexts_to_sequences(texts): Converts each text into a sequence of numbers.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nMAX_WORDS = vocab_size\nMAX_SEQUENCE_LENGTH = 100  # Use 95th percentile length :86\nSTRIDE =50   # Fix stride calculation\nEMBEDDING_DIM = 300  # Keep the same unless changing embeddings\n\n\ntexts = df[\"comment_text\"].astype(str).values  # Convert comments to string\nsequences = tokenizer.texts_to_sequences(texts)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:57:47.913525Z","iopub.execute_input":"2025-03-29T15:57:47.913894Z","iopub.status.idle":"2025-03-29T15:59:57.97665Z","shell.execute_reply.started":"2025-03-29T15:57:47.913864Z","shell.execute_reply":"2025-03-29T15:59:57.975454Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sliding Window Function\ndef create_sliding_window_sequences(sequences, max_len=MAX_SEQUENCE_LENGTH, stride=50):\n    all_chunks = []\n    for seq in sequences:\n        chunks = []\n        if len(seq) <= max_len:\n            seq += [0] * (max_len - len(seq))\n            chunks.append(seq)\n        else:\n            for i in range(0, len(seq) - max_len + 1, stride):\n                chunks.append(seq[i : i + max_len])\n            if (len(seq) - max_len) % stride != 0:\n                chunks.append(seq[-max_len:])\n        all_chunks.extend(chunks)\n    return np.array(all_chunks)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T15:59:58.801963Z","iopub.execute_input":"2025-03-29T15:59:58.802384Z","iopub.status.idle":"2025-03-29T15:59:58.809267Z","shell.execute_reply.started":"2025-03-29T15:59:58.802344Z","shell.execute_reply":"2025-03-29T15:59:58.808157Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Apply sliding window\nX_sliding = create_sliding_window_sequences(sequences)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T16:00:05.239289Z","iopub.execute_input":"2025-03-29T16:00:05.239659Z","iopub.status.idle":"2025-03-29T16:01:04.550457Z","shell.execute_reply.started":"2025-03-29T16:00:05.239627Z","shell.execute_reply":"2025-03-29T16:01:04.5494Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Ensure Y is a 1D array\nY = df[labels].values  # Shape: (4703664, 6)\n\n# Count how many chunks per original sequence\nnum_chunks_per_sequence = [len(create_sliding_window_sequences([seq])) for seq in sequences]\n\n# Repeat labels correctly\nY_expanded = np.repeat(Y, num_chunks_per_sequence, axis=0)\n\n# Verify shapes again\nprint(f\"Fixed Shapes - X_sliding: {X_sliding.shape}, Y_expanded: {Y_expanded.shape}\")\nassert Y_expanded.shape[0] == X_sliding.shape[0], \"Still mismatched!\"\n# Debugging Output\nprint(f\"Total Chunks Expected: {sum(num_chunks_per_sequence)}\")\nprint(f\"X_sliding Shape: {X_sliding.shape[0]}\")\nprint(f\"Y Shape: {Y.shape[0]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T16:01:06.646851Z","iopub.execute_input":"2025-03-29T16:01:06.647195Z","iopub.status.idle":"2025-03-29T16:01:53.409674Z","shell.execute_reply.started":"2025-03-29T16:01:06.647168Z","shell.execute_reply":"2025-03-29T16:01:53.40869Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train-test split\nX_train, X_test, Y_train, Y_test = train_test_split(X_sliding, Y_expanded, test_size=0.2, random_state=42)\nprint(f\"Training Data Shape: {X_train.shape}, Labels: {Y_train.shape}\")\nprint(f\"Testing Data Shape: {X_test.shape}, Labels: {Y_test.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T16:01:55.460574Z","iopub.execute_input":"2025-03-29T16:01:55.460878Z","iopub.status.idle":"2025-03-29T16:01:59.143322Z","shell.execute_reply.started":"2025-03-29T16:01:55.460851Z","shell.execute_reply":"2025-03-29T16:01:59.142194Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"del sequences\ndel tokenizer\ndel texts\ndel df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T16:02:01.226652Z","iopub.execute_input":"2025-03-29T16:02:01.226941Z","iopub.status.idle":"2025-03-29T16:02:04.08732Z","shell.execute_reply.started":"2025-03-29T16:02:01.226915Z","shell.execute_reply":"2025-03-29T16:02:04.086162Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc  # Garbage collector\n\n# Delete unnecessary large variables\ndel X_sliding, num_chunks_per_sequence # Large lists/arrays\n\n# Run garbage collection\ngc.collect()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T16:02:04.105449Z","iopub.execute_input":"2025-03-29T16:02:04.105815Z","iopub.status.idle":"2025-03-29T16:02:04.427332Z","shell.execute_reply.started":"2025-03-29T16:02:04.105777Z","shell.execute_reply":"2025-03-29T16:02:04.426163Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\n# 5️⃣ Load the best model\nmodel = tf.keras.models.load_model(\"/kaggle/input/hatedetectionbilstm/keras/default/1/biLSTM.keras\")\n\n# 6️⃣ Evaluate the model\ntest_loss, test_acc = model.evaluate(X_test, Y_test)\nprint(f\"Test Accuracy: {test_acc:.4f}, Test Loss: {test_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-29T16:02:04.446768Z","iopub.execute_input":"2025-03-29T16:02:04.447176Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nprint(\"Label Counts in Y_test:\", np.sum(Y_test, axis=0))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n# Get model predictions (probabilities)\ny_pred_probs = model.predict(X_test)\n\n# Convert probabilities to binary labels (0 or 1) using threshold = 0.5\ny_pred = (y_pred_probs >= 0.5).astype(int)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for idx, label in enumerate(labels):\n    print(f\"{label}: True = {np.sum(Y_test[:, idx])}, Predicted = {np.sum(y_pred[:, idx])}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\n\nprint(classification_report(Y_test, y_pred, target_names=labels, zero_division=0))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_all_zero_labels = np.sum(np.all(Y_test == 0, axis=1))\nprint(f\"Number of samples in Y_test with no toxic labels: {num_all_zero_labels}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_all_zero_predictions = np.sum(np.all(y_pred == 0, axis=1))\nprint(f\"Number of samples where the model predicted no toxic labels: {num_all_zero_predictions}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Predicted 'identity_hate' counts:\", np.sum(y_pred[:, 5]))\nprint(\"Actual 'identity_hate' counts:\", np.sum(Y_test[:, 5]))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\nlabel_counts = np.sum(Y_test, axis=0)  # Count occurrences for each label\nfor i, label in enumerate(labels):\n    print(f\"Number of samples for {label}: {label_counts[i]}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\nfor i, label in enumerate(labels):\n    acc = accuracy_score(Y_test[:, i], y_pred[:, i])  # Calculate accuracy per label\n    print(f\"Accuracy for {label}: {acc:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import multilabel_confusion_matrix\n\n# Compute confusion matrices for each label\ncm = multilabel_confusion_matrix(Y_test, y_pred)\n\n# Define label names\nlabels = [\"toxic\", \"severe_toxic\", \"obscene\", \"insult\", \"threat\", \"identity_hate\"]\n\n# Plot all confusion matrices\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))  # 2 rows, 3 columns\naxes = axes.ravel()\n\nfor i in range(6):\n    sns.heatmap(cm[i], annot=True, fmt='d', cmap=\"Blues\", ax=axes[i])\n    axes[i].set_title(f\"Confusion Matrix for {labels[i]}\")\n    axes[i].set_xlabel(\"Predicted Labels\")\n    axes[i].set_ylabel(\"True Labels\")\n    axes[i].set_xticklabels([\"Not Present\", \"Present\"])\n    axes[i].set_yticklabels([\"Not Present\", \"Present\"])\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\nauc_scores = roc_auc_score(Y_test, y_pred_probs, average=None)\nfor i, label in enumerate(labels):\n    print(f\"ROC-AUC for {label}: {auc_scores[i]:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 7))\n\nfor i, label in enumerate(labels):\n    fpr, tpr, _ = roc_curve(Y_test[:, i], y_pred_probs[:, i])  # Get FPR & TPR\n    roc_auc = auc(fpr, tpr)  # Compute AUC\n    plt.plot(fpr, tpr, label=f\"{label} (AUC = {roc_auc:.4f})\")  # Plot ROC Curve\n\n# Plot settings\nplt.plot([0, 1], [0, 1], color=\"grey\", linestyle=\"--\")  # Diagonal line\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curves for All Labels\")\nplt.legend(loc=\"lower right\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\nroc_auc = roc_auc_score(Y_test, y_pred_probs, average=\"macro\")  # Use probs, not binary labels\nprint(f\"ROC-AUC Score: {roc_auc:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save test predictions (probabilities)\n#np.save(\"BiLSTM_test_preds.npy\", y_pred_probs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save training predictions (needed for stacking)\n#y_train_probs = model.predict(X_train)\n#np.save(\"BiLSTM_train_preds.npy\", y_train_probs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save train & test labels\n#np.save(\"BiLSTM_Y_train.npy\", Y_train)\n#np.save(\"BiLSTM_Y_test.npy\", Y_test)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}