{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11111498,"sourceType":"datasetVersion","datasetId":6926583},{"sourceId":11120959,"sourceType":"datasetVersion","datasetId":6934896},{"sourceId":11130665,"sourceType":"datasetVersion","datasetId":6941960}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-22T22:31:13.900362Z","iopub.execute_input":"2025-03-22T22:31:13.9007Z","iopub.status.idle":"2025-03-22T22:31:14.226886Z","shell.execute_reply.started":"2025-03-22T22:31:13.900665Z","shell.execute_reply":"2025-03-22T22:31:14.225917Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\n\n# List all available GPUs\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        # Set memory growth to avoid using all GPU memory at once\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(\"Using GPU:\", gpus)\n    except RuntimeError as e:\n        print(\"Error:\", e)\nelse:\n    print(\"No GPU detected. Running on CPU.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T23:21:46.300036Z","iopub.execute_input":"2025-03-22T23:21:46.300328Z","iopub.status.idle":"2025-03-22T23:21:58.493245Z","shell.execute_reply.started":"2025-03-22T23:21:46.300299Z","shell.execute_reply":"2025-03-22T23:21:58.492483Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Input, Embedding, LSTM, Bidirectional, Dense, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T23:21:58.494354Z","iopub.execute_input":"2025-03-22T23:21:58.494974Z","iopub.status.idle":"2025-03-22T23:21:59.391001Z","shell.execute_reply.started":"2025-03-22T23:21:58.494949Z","shell.execute_reply":"2025-03-22T23:21:59.390104Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load dataset\ndf = pd.read_csv(\"/kaggle/input/jigsaw-competition-merged-and-balanced/cleaned_data.csv\")\nlabels = [\"toxic\", \"severe_toxic\", \"obscene\", \"insult\", \"threat\", \"identity_hate\"]  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T23:21:59.39244Z","iopub.execute_input":"2025-03-22T23:21:59.393807Z","iopub.status.idle":"2025-03-22T23:22:24.982359Z","shell.execute_reply.started":"2025-03-22T23:21:59.393781Z","shell.execute_reply":"2025-03-22T23:22:24.98145Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T23:22:24.983633Z","iopub.execute_input":"2025-03-22T23:22:24.983928Z","iopub.status.idle":"2025-03-22T23:22:25.004986Z","shell.execute_reply.started":"2025-03-22T23:22:24.983906Z","shell.execute_reply":"2025-03-22T23:22:25.004259Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.columns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T22:31:50.806524Z","iopub.execute_input":"2025-03-22T22:31:50.806772Z","iopub.status.idle":"2025-03-22T22:31:50.812159Z","shell.execute_reply.started":"2025-03-22T22:31:50.806741Z","shell.execute_reply":"2025-03-22T22:31:50.811299Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df=df.dropna()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T22:31:50.813097Z","iopub.execute_input":"2025-03-22T22:31:50.813347Z","iopub.status.idle":"2025-03-22T22:31:51.531398Z","shell.execute_reply.started":"2025-03-22T22:31:50.813327Z","shell.execute_reply":"2025-03-22T22:31:51.530506Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\n\n# Function to check if a sentence contains special characters\ndef has_special_chars(text):\n    return bool(re.search(r'[^a-zA-Z0-9\\s]', text))  # Returns True if special chars exist\n\n# Filter sentences with special characters\nspecial_char_sentences = df[df['comment_text'].apply(has_special_chars)]\n\n# Display them\nprint(special_char_sentences)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T22:31:51.533813Z","iopub.execute_input":"2025-03-22T22:31:51.534062Z","iopub.status.idle":"2025-03-22T22:32:01.992249Z","shell.execute_reply.started":"2025-03-22T22:31:51.534042Z","shell.execute_reply":"2025-03-22T22:32:01.991502Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\n\n# Function to remove special characters\ndef remove_special_chars(text):\n    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Keeps only letters, numbers, and spaces\n\n# Apply to your dataframe\ndf['comment_text'] = df['comment_text'].astype(str).apply(remove_special_chars)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T23:22:25.005636Z","iopub.execute_input":"2025-03-22T23:22:25.005904Z","iopub.status.idle":"2025-03-22T23:22:36.817602Z","shell.execute_reply.started":"2025-03-22T23:22:25.005873Z","shell.execute_reply":"2025-03-22T23:22:36.816918Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\n\n# Load the tokenizer\nwith open('/kaggle/input/hatedetectiontokenizer/tokenizer.pkl', 'rb') as f:\n    tokenizer = pickle.load(f)\n\n# Confirm tokenizer loaded correctly\nprint(f\"Tokenizer vocabulary size: {len(tokenizer.word_index)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T23:22:36.818372Z","iopub.execute_input":"2025-03-22T23:22:36.818689Z","iopub.status.idle":"2025-03-22T23:22:37.646076Z","shell.execute_reply.started":"2025-03-22T23:22:36.818659Z","shell.execute_reply":"2025-03-22T23:22:37.645281Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n# Load the precomputed embedding matrix\nembedding_matrix = np.load('/kaggle/input/embeddingmatrix/embedding_matrix.npy')\n\n# Check the shape\nprint(f\"Embedding Matrix Shape: {embedding_matrix.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T23:22:37.646829Z","iopub.execute_input":"2025-03-22T23:22:37.647064Z","iopub.status.idle":"2025-03-22T23:22:43.589335Z","shell.execute_reply.started":"2025-03-22T23:22:37.647043Z","shell.execute_reply":"2025-03-22T23:22:43.588611Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T23:22:43.591497Z","iopub.execute_input":"2025-03-22T23:22:43.591756Z","iopub.status.idle":"2025-03-22T23:22:43.595366Z","shell.execute_reply.started":"2025-03-22T23:22:43.591734Z","shell.execute_reply":"2025-03-22T23:22:43.594718Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vocab_size=478019\nprint(f\"Vocabulary size: {vocab_size}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T23:22:43.596806Z","iopub.execute_input":"2025-03-22T23:22:43.597084Z","iopub.status.idle":"2025-03-22T23:22:43.615792Z","shell.execute_reply.started":"2025-03-22T23:22:43.597055Z","shell.execute_reply":"2025-03-22T23:22:43.615095Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"fit_on_texts(texts): Learns word mappings and counts vocabulary size.\ntexts_to_sequences(texts): Converts each text into a sequence of numbers.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nMAX_WORDS = vocab_size\nMAX_SEQUENCE_LENGTH = 100  # Use 95th percentile length :86\nSTRIDE =50   # Fix stride calculation\nEMBEDDING_DIM = 300  # Keep the same unless changing embeddings\n\n\ntexts = df[\"comment_text\"].astype(str).values  # Convert comments to string\nsequences = tokenizer.texts_to_sequences(texts)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T23:22:43.616465Z","iopub.execute_input":"2025-03-22T23:22:43.616725Z","iopub.status.idle":"2025-03-22T23:24:15.827037Z","shell.execute_reply.started":"2025-03-22T23:22:43.616693Z","shell.execute_reply":"2025-03-22T23:24:15.82625Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sliding Window Function\ndef create_sliding_window_sequences(sequences, max_len=MAX_SEQUENCE_LENGTH, stride=50):\n    all_chunks = []\n    for seq in sequences:\n        chunks = []\n        if len(seq) <= max_len:\n            seq += [0] * (max_len - len(seq))\n            chunks.append(seq)\n        else:\n            for i in range(0, len(seq) - max_len + 1, stride):\n                chunks.append(seq[i : i + max_len])\n            if (len(seq) - max_len) % stride != 0:\n                chunks.append(seq[-max_len:])\n        all_chunks.extend(chunks)\n    return np.array(all_chunks)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T23:24:15.827831Z","iopub.execute_input":"2025-03-22T23:24:15.828051Z","iopub.status.idle":"2025-03-22T23:24:15.833311Z","shell.execute_reply.started":"2025-03-22T23:24:15.828031Z","shell.execute_reply":"2025-03-22T23:24:15.832453Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Apply sliding window\nX_sliding = create_sliding_window_sequences(sequences)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T23:24:15.834008Z","iopub.execute_input":"2025-03-22T23:24:15.834285Z","iopub.status.idle":"2025-03-22T23:25:01.156401Z","shell.execute_reply.started":"2025-03-22T23:24:15.834264Z","shell.execute_reply":"2025-03-22T23:25:01.15572Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Ensure Y is a 1D array\nY = df[labels].values  # Shape: (4703664, 6)\n\n# Count how many chunks per original sequence\nnum_chunks_per_sequence = [len(create_sliding_window_sequences([seq])) for seq in sequences]\n\n# Repeat labels correctly\nY_expanded = np.repeat(Y, num_chunks_per_sequence, axis=0)\n\n# Verify shapes again\nprint(f\"Fixed Shapes - X_sliding: {X_sliding.shape}, Y_expanded: {Y_expanded.shape}\")\nassert Y_expanded.shape[0] == X_sliding.shape[0], \"Still mismatched!\"\n# Debugging Output\nprint(f\"Total Chunks Expected: {sum(num_chunks_per_sequence)}\")\nprint(f\"X_sliding Shape: {X_sliding.shape[0]}\")\nprint(f\"Y Shape: {Y.shape[0]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T23:25:01.167933Z","iopub.execute_input":"2025-03-22T23:25:01.168167Z","iopub.status.idle":"2025-03-22T23:25:38.922075Z","shell.execute_reply.started":"2025-03-22T23:25:01.168148Z","shell.execute_reply":"2025-03-22T23:25:38.921226Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train-test split\nX_train, X_test, Y_train, Y_test = train_test_split(X_sliding, Y_expanded, test_size=0.2, random_state=42)\nprint(f\"Training Data Shape: {X_train.shape}, Labels: {Y_train.shape}\")\nprint(f\"Testing Data Shape: {X_test.shape}, Labels: {Y_test.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T23:25:38.922948Z","iopub.execute_input":"2025-03-22T23:25:38.923264Z","iopub.status.idle":"2025-03-22T23:25:41.480091Z","shell.execute_reply.started":"2025-03-22T23:25:38.923232Z","shell.execute_reply":"2025-03-22T23:25:41.479314Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"del sequences\ndel tokenizer\ndel texts\ndel df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T23:25:41.480813Z","iopub.execute_input":"2025-03-22T23:25:41.481099Z","iopub.status.idle":"2025-03-22T23:25:43.506846Z","shell.execute_reply.started":"2025-03-22T23:25:41.481074Z","shell.execute_reply":"2025-03-22T23:25:43.50587Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Embedding, GRU, Bidirectional, Dense, Dropout,LSTM\nfrom tensorflow.keras.models import Model\n\ndef BiLSTM():\n    # Text Input\n    input_text = Input(shape=(MAX_SEQUENCE_LENGTH,))\n\n    # Embedding Layer (Make sure embedding_matrix is loaded)\n    embedding_layer = Embedding(\n        input_dim=MAX_WORDS,\n        output_dim=EMBEDDING_DIM,\n        weights=[embedding_matrix],  # Preloaded GloVe/FastText embeddings\n        trainable=False\n    )(input_text)\n\n    # BiLSTM Model\n    x = Bidirectional(LSTM(128, return_sequences=True))(embedding_layer)\n    x = LSTM(64)(x)\n\n    # Fully Connected Layers\n    x = Dense(128, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    \n    # Output Layer (Multilabel classification)\n    output = Dense(len(labels), activation='sigmoid')(x)\n\n    # Create & Compile Model\n    model = Model(inputs=input_text, outputs=output)\n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T23:25:43.507871Z","iopub.execute_input":"2025-03-22T23:25:43.508221Z","iopub.status.idle":"2025-03-22T23:25:43.522391Z","shell.execute_reply.started":"2025-03-22T23:25:43.508184Z","shell.execute_reply":"2025-03-22T23:25:43.521538Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.callbacks import Callback\nimport tensorflow as tf\n\ncallbacks = [\n    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2,mode=\"min\",  # Want the lowest loss\n    restore_best_weights=True,  # Load best model after stopping\n    verbose=1),\n    tf.keras.callbacks.ModelCheckpoint(\"best.keras\",save_best_only=True,monitor='val_loss', mode='min')\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T23:25:43.523397Z","iopub.execute_input":"2025-03-22T23:25:43.523633Z","iopub.status.idle":"2025-03-22T23:25:43.538599Z","shell.execute_reply.started":"2025-03-22T23:25:43.523612Z","shell.execute_reply":"2025-03-22T23:25:43.537803Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create Model\nmodel = BiLSTM()\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T23:25:43.539441Z","iopub.execute_input":"2025-03-22T23:25:43.539768Z","iopub.status.idle":"2025-03-22T23:25:46.057841Z","shell.execute_reply.started":"2025-03-22T23:25:43.53974Z","shell.execute_reply":"2025-03-22T23:25:46.056135Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc  # Garbage collector\n\n# Delete unnecessary large variables\ndel X_sliding, num_chunks_per_sequence # Large lists/arrays\n\n# Run garbage collection\ngc.collect()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T23:25:46.059263Z","iopub.execute_input":"2025-03-22T23:25:46.059814Z","iopub.status.idle":"2025-03-22T23:25:46.471684Z","shell.execute_reply.started":"2025-03-22T23:25:46.059768Z","shell.execute_reply":"2025-03-22T23:25:46.470327Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history=model.fit(\n    X_train, Y_train,\n    epochs=15,\n    validation_split=0.2,\n    batch_size=32,  # Adjust based on memory\n    shuffle=True,   # Ensures random order during training\n    callbacks=callbacks,\n    verbose=1,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T23:25:46.47286Z","iopub.execute_input":"2025-03-22T23:25:46.473224Z","execution_failed":"2025-03-23T02:51:51.341Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\n\n# Get model predictions (probabilities)\ny_pred_probs = model.predict(X_test)\n\n# Convert probabilities to binary labels (0 or 1) using threshold = 0.5\ny_pred = (y_pred_probs >= 0.5).astype(int)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-22T23:20:31.341Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\nfor i, label in enumerate(labels):\n    acc = accuracy_score(Y_test[:, i], y_pred[:, i])  # Calculate accuracy per label\n    print(f\"Accuracy for {label}: {acc:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import multilabel_confusion_matrix\n\n# Compute confusion matrices for each label\ncm = multilabel_confusion_matrix(Y_test, y_pred)\n\n# Define label names\nlabels = [\"toxic\", \"severe_toxic\", \"obscene\", \"insult\", \"threat\", \"identity_hate\"]\n\n# Plot all confusion matrices\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))  # 2 rows, 3 columns\naxes = axes.ravel()\n\nfor i in range(6):\n    sns.heatmap(cm[i], annot=True, fmt='d', cmap=\"Blues\", ax=axes[i])\n    axes[i].set_title(f\"Confusion Matrix for {labels[i]}\")\n    axes[i].set_xlabel(\"Predicted Labels\")\n    axes[i].set_ylabel(\"True Labels\")\n    axes[i].set_xticklabels([\"Not Present\", \"Present\"])\n    axes[i].set_yticklabels([\"Not Present\", \"Present\"])\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint(\"Classification Report:\\n\", classification_report(Y_test, y_pred, target_names=labels, zero_division=0))\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-22T23:20:31.342Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\n\n# Generate confusion matrix\nconf_matrix = confusion_matrix(Y_test.argmax(axis=1), y_pred.argmax(axis=1))\n\n# Plot confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels)\nplt.xlabel(\"Predicted Labels\")\nplt.ylabel(\"True Labels\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-22T23:20:31.342Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\nauc_scores = roc_auc_score(Y_test, y_pred_probs, average=None)\nfor i, label in enumerate(labels):\n    print(f\"ROC-AUC for {label}: {auc_scores[i]:.4f}\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-22T23:20:31.342Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 7))\n\nfor i, label in enumerate(labels):\n    fpr, tpr, _ = roc_curve(Y_test[:, i], y_pred_probs[:, i])  # Get FPR & TPR\n    roc_auc = auc(fpr, tpr)  # Compute AUC\n    plt.plot(fpr, tpr, label=f\"{label} (AUC = {roc_auc:.4f})\")  # Plot ROC Curve\n\n# Plot settings\nplt.plot([0, 1], [0, 1], color=\"grey\", linestyle=\"--\")  # Diagonal line\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curves for All Labels\")\nplt.legend(loc=\"lower right\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\nroc_auc = roc_auc_score(Y_test, y_pred_probs, average=\"macro\")  # Use probs, not binary labels\nprint(f\"ROC-AUC Score: {roc_auc:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# Save the trained model in .keras format\nmodel.save(\"biLSTM.keras\")  # Corrected line\n\n# Save train & test labels\nnp.save(\"BiLSTM_Y_train.npy\", Y_train)\nnp.save(\"BiLSTM_Y_test.npy\", Y_test)\n\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-22T23:20:31.342Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save test predictions (probabilities)\nnp.save(\"biLSTM_test_preds.npy\", y_pred_probs)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save training predictions (needed for stacking)\ny_train_probs = model.predict(X_train)\nnp.save(\"biLSTM_train_preds.npy\", y_train_probs)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}